{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bdeb4c-7631-4262-87b9-e74c79518107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f2e09-b0aa-4764-b3c2-954f73615363",
   "metadata": {},
   "source": [
    "# 用微调好的模型去部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d79509-3dfd-45cd-a71b-3114362f2d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-07-29 16:52:11 Starting - Preparing the instances for training\n",
      "2024-07-29 16:52:11 Downloading - Downloading the training image\n",
      "2024-07-29 16:52:11 Training - Training image download completed. Training in progress.\n",
      "2024-07-29 16:52:11 Uploading - Uploading generated training model\n",
      "2024-07-29 16:52:11 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "training_job_name = \"jumpstart-lasson-llama2-0729-epoch-6-1\"\n",
    "model_id = \"meta-textgeneration-llama-2-7b-f\"\n",
    "\n",
    "model = JumpStartEstimator.attach(training_job_name, model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e7dd89-8ae3-4bc6-911a-9c25f05565f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "instance_type=\"ml.g5.2xlarge\"\n",
    "predictor = model.deploy(instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237e9f93-be8a-4882-ba2a-a5e8cae16ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.10.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a339b29-7947-4bef-a6a0-89e00cf3761a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " Synonyms: [\"Papilledema\",\"Swollen discs\",\"Indistinct margins\",\"Blurred disc margins\",\"Suspicious discs\",\"Disc swelling\",\"Optic nerve swelling\"]\n",
      "\n",
      "PseudoSynonyms: [\"Pseudopapilledema\",\"Drusen\",\"Tilted disc\",\"Anomalous discs\"]\n",
      "\n",
      "\n",
      "Role : You are a experienced doctor who have memory of electronic medical records related to many diseases.\n",
      "\n",
      "Instruction : please extract the referral content from the following referral letter  separeted by ###. \n",
      "\n",
      "output your result directly in format: \"is_Papilledema\": boolean, \"referral_content\": \"\".\n",
      "\n",
      "Rule For is_Papilledema : If the referral letter contains one of words in Synonyms, then is_Papilledema = true; If the letter contains words in PseudoSynonyms or doesn't contain words in Synonyms, then is_Papilledema = False.\n",
      "\n",
      "Rule For referral_content : this content should be a whole paragraph which tells Patient need referral. If the referral_letter contains this content, you should include it. If the letter doesn't contain related information, then it should be null.\n",
      "\n",
      "###\n",
      "Dear Dr. Smith,\n",
      "\n",
      "I am writing to refer Ms. Jane Doe, a 61-year-old woman who has presented with a 2-week history of progressive shortness of breath, non-productive cough, and lower extremity swelling.\n",
      "\n",
      "Patient Information:\n",
      "Name: Jane Doe\n",
      "Age: 61\n",
      "Gender: Female\n",
      "Symptoms: Shortness of breath, non-productive cough, lower extremity swelling\n",
      "Vital Signs: Tachycardia (114 beats per min), Tachypnea (28 breaths per min), Hypoxemia (88% SaO2 on room air)\n",
      "Physical Exam Findings: Jugular venous distention, bibasilar crackles, bilateral lower extremity pitting edema, apical holosystolic murmur with a mid-diastolic click\n",
      "\n",
      "The reason for this referral is to further evaluate and manage Ms. Jane Doe's progressively worsening cardiopulmonary symptoms. Given her clinical presentation, further investigation is essential to rule out possible cardiac etiologies, including congestive heart failure or valvular disease.\n",
      "\n",
      "Thank you for your attention to this matter.\n",
      "\n",
      "Sincerely,\n",
      "Dr. John Doe\n",
      "###\n",
      "\n",
      "### Response:\n",
      "\"is_Papilledema\": False, \"referral_content\": \"The reason for this referral is to further evaluate and manage Ms. Jane Doe's progressively worsening cardiopulmonary symptoms. Given her clinical presentation, further investigation is essential to rule out possible cardiac etiologies, including congestive heart failure or valvular disease.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./letter1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    letter = f.read()\n",
    "# print(letter)\n",
    "\n",
    "response = predictor.predict({'inputs': letter,\n",
    "                             'parameters': {'max_new_tokens': 128}})\n",
    "\n",
    "print(\"Output:\\n\", response[0][\"generated_text\"].strip(), end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "530703cf-b6a2-4aea-a478-8d2cfca66096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "import sacrebleu\n",
    "import re\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Function to replace output prefix\n",
    "def replace_output_prefix(input_str):\n",
    "    match = re.search(r'Response:\\s*(.*)', input_str, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return input_str\n",
    "\n",
    "# Function to extract referral content\n",
    "def extract_referral_content(tmp_str):\n",
    "    patterns = [\n",
    "        r'\"referral_content\"\\s*:\\s*\"(.*?)\"',\n",
    "        r'\\bThe reason for this referral is\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bThe referral is being made because\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bReferral Reason\\b\\s*:\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bThe reason for referral is\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bfor referral is\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bneed referral because\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bThis referral is made to\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bWe are referring this patient because\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bThe purpose of this referral is\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bOur reason for referral is\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bDue to\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bbecause of\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bAs a result of\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\bThe intent of this referral is\\b\\s*(.*?)(?<!\\s)\\.',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, tmp_str)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "\n",
    "    sentences = tmp_str.split('. ')\n",
    "    for sentence in sentences:\n",
    "        if 'referral' in sentence.lower():\n",
    "            return sentence.strip()\n",
    "    \n",
    "    return \"extract failure\"\n",
    "\n",
    "# Function to clean extracted text\n",
    "def clean_extracted_text(text):\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Function to process a single test data entry\n",
    "def process_single_test(single_test, predictor):\n",
    "    instruction = single_test.get(\"instruction\", \"\")\n",
    "    whole_letter = single_test.get(\"whole_letter\", \"\")\n",
    "    referral_content = single_test.get(\"referral_content\", \"\")\n",
    "    \n",
    "    # Improved prompt with more structure and clear delimiters\n",
    "    prompt = (\n",
    "        f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "    )\n",
    "    \n",
    "    response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 512}})\n",
    "    \n",
    "    reference_text = referral_content if referral_content is not None else \"\"\n",
    "    candidate_text = \"extract failure\"\n",
    "    try:\n",
    "        response_text = response[0][\"generated_text\"].strip()\n",
    "       # logging.info(f\"Raw response: {response_text}\")\n",
    "        tmp_str = replace_output_prefix(response_text)\n",
    "        #logging.info(f\"Processed response: {tmp_str}\")\n",
    "        candidate_text = extract_referral_content(tmp_str)\n",
    "        #logging.info(f\"Extracted referral content: {candidate_text}\")\n",
    "        candidate_text = clean_extracted_text(candidate_text)\n",
    "    except Exception as err:\n",
    "        #logging.error(f\"Error processing ID: {single_test.get('id', 'unknown')}\")\n",
    "        #logging.error(f\"Response: {response[0]['generated_text'].strip() if response else 'No response'}\")\n",
    "        logging.error(err)\n",
    "    finally:\n",
    "        bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "        single_test[\"bleu\"] = bleu.score\n",
    "        single_test[\"predict_referral_content\"] = candidate_text\n",
    "        return single_test, bleu.score\n",
    "\n",
    "# Evaluation function with optimized processing\n",
    "def evaluate_jsonl_with_llama2(predictor, path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(executor.map(lambda x: process_single_test(x, predictor), test_data_json))\n",
    "    \n",
    "    # Separate the results and BLEU scores\n",
    "    processed_data, bleu_scores = zip(*results)\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return processed_data, list(bleu_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6d6e530-83a0-4519-af02-e710554e9ded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:CSV file has been saved to lewis-deploy_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"lewis-deploy_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59fdc28b-ce0c-4556-9c60-b464c68c2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predict_data(bleu_score_list):\n",
    "    # bleu score >= 100\n",
    "    count_gt_100 = sum(1 for score in bleu_score_list if score >= 100)\n",
    "\n",
    "    # bleu score >=70\n",
    "    count_gt_70 = sum(1 for score in bleu_score_list if score > 70)\n",
    "\n",
    "    prob_gt_100 = count_gt_100 / len(bleu_score_list)\n",
    "    prob_gt_70 = count_gt_70 / len(bleu_score_list)\n",
    "    average_score = sum(bleu_score_list) / float(len(bleu_score_list))\n",
    "\n",
    "    print(f\"Count of bleu score >=100：{count_gt_100}, % of bleu scores >=100 ： {prob_gt_100}\")\n",
    "    print(f\"Count of bleu score >=70：{count_gt_70}, % of bleu scores >=70： {prob_gt_70}\")\n",
    "    print(f\"Average bleu score: {average_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9479c7c-0ff1-47a2-a7b8-2129b9829bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of bleu score >=100：125, % of bleu scores >=100 ： 0.8928571428571429\n",
      "Count of bleu score >=70：131, % of bleu scores >=70： 0.9357142857142857\n",
      "Average bleu score: 94.21227555894221\n"
     ]
    }
   ],
   "source": [
    "analyze_predict_data(test_bleu_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562acc3-cbc6-467e-8571-5296899eef4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ee165-179f-41d3-9130-84062d80891c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8ebe6-20a3-4665-aba3-f634d814f254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a941a3-efe5-413e-8d05-c835a6eeb131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bce1d-21fc-48c6-8ab6-9e90e62d6315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21198f9f-2dd8-4750-b7fa-8b8f9cf59a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a0012-1c9f-4538-8a93-6dafb9977b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014f87f-44c4-40fa-b687-8eef6d9ea028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05637e40-cd22-4a6a-8fdb-f0e5b594263e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a37760d8-7b71-4c4f-baf7-256d0251aaa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    bleu_score_list = []\n",
    "\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test[\"instruction\"]\n",
    "        whole_letter = single_test[\"whole_letter\"]\n",
    "        referral_content = single_test[\"referral_content\"]\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt,\n",
    "                                 'parameters': {'max_new_tokens': 256}})\n",
    "        # print(prompt)\n",
    "        reference_text = referral_content\n",
    "        try:\n",
    "            tmp = json.loads(response[0][\"generated_text\"].strip())\n",
    "            candidate_text = tmp[\"referral_content\"]\n",
    "        except Exception as err:\n",
    "            print(single_test[\"id\"])\n",
    "            print(response[0][\"generated_text\"].strip())\n",
    "            print()\n",
    "            candidate_text = \"extract failure\"\n",
    "        finally:\n",
    "\n",
    "            evaluate_list.append(candidate_text)\n",
    "            # print(\"predict: \" + candidate_text)\n",
    "            # print(\"real: \" + reference_text)\n",
    "\n",
    "\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            # print(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "\n",
    "            \n",
    "#     with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "#         for single_test in test_data_json:\n",
    "#             f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "\n",
    "#     print(f\"predicted data has been saved to {output_path}.\")\n",
    "    \n",
    "    # 创建 CSV 文件\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test[\"id\"],\n",
    "            \"name\": single_test[\"name\"],\n",
    "            \"instruction\": single_test[\"instruction\"],\n",
    "            \"whole_letter\": single_test[\"whole_letter\"],\n",
    "            \"referral_content\": single_test[\"referral_content\"],\n",
    "            \"predict_referral_content\": single_test[\"predict_referral_content\"],\n",
    "            \"bleu\": single_test[\"bleu\"],\n",
    "        })\n",
    "\n",
    "    # 创建 DataFrame\n",
    "    df = pd.DataFrame(csv_data)\n",
    "\n",
    "    # 保存为 CSV 文件\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9539a78d-d944-42b9-9c79-e018def71605",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list,test_rouge_score_list = evaluate_jsonl_with_llama2(predictor, \"./test_dir/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da5e12a9-34b1-441e-82f1-852877168e52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure', 'extract failure']\n"
     ]
    }
   ],
   "source": [
    "print(test_evaluate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0a271d6-aa3f-4665-bc33-b89ef89aeaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112776c5-785b-432f-b76b-7e030335c873",
   "metadata": {},
   "source": [
    "# 用原始llama2-7b-f去部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c2aa286-c56c-4e60-8bdf-ed7150d6491a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HubContentType' from 'sagemaker.jumpstart.types' (/opt/conda/lib/python3.10/site-packages/sagemaker/jumpstart/types.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjumpstart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JumpStartModel\n\u001b[1;32m      3\u001b[0m pretrain_model_id, pretrain_model_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-textgeneration-llama-2-7b-f\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.*\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m pretrain_model \u001b[38;5;241m=\u001b[39m JumpStartModel(model_id\u001b[38;5;241m=\u001b[39mpretrain_model_id, model_version\u001b[38;5;241m=\u001b[39mpretrain_model_version, instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml.g5.2xlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/jumpstart/model.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainer_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExplainerConfig\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjumpstart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JumpStartModelsAccessor\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjumpstart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_hub_arn_for_init_kwargs\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjumpstart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JumpStartScriptScope\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjumpstart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     INVALID_MODEL_ID_ERROR_MSG,\n\u001b[1;32m     32\u001b[0m     get_proprietary_model_subscription_error,\n\u001b[1;32m     33\u001b[0m     get_proprietary_model_subscription_msg,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/jumpstart/hub/utils.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Session\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aws_partition\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjumpstart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HubContentType, HubArnExtractedInfo\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjumpstart\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecifiers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpecifierSet, InvalidSpecifier\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HubContentType' from 'sagemaker.jumpstart.types' (/opt/conda/lib/python3.10/site-packages/sagemaker/jumpstart/types.py)"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrain_model_id, pretrain_model_version = \"meta-textgeneration-llama-2-7b-f\", \"3.*\"\n",
    "\n",
    "pretrain_model = JumpStartModel(model_id=pretrain_model_id, model_version=pretrain_model_version, instance_type=\"ml.g5.2xlarge\" )\n",
    "\n",
    "pretrain_predictor = pretrain_model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04536720-7adb-4041-9ef5-f41049f15a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
