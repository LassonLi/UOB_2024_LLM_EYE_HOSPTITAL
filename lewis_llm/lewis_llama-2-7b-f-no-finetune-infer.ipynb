{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dee0554-9670-4ede-8acc-cedbc0950393",
   "metadata": {},
   "source": [
    "# Run Llama 2 Models in SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b84c5-6148-461a-b95b-63d2d4b1d783",
   "metadata": {},
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy a JumpStart model for Text Generation using the Llama 2 fine-tuned model optimized for dialogue use cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d458cf0-02e2-4066-927b-25fa5ef2a07e",
   "metadata": {},
   "source": [
    "***\n",
    "You can continue with the default model or choose a different model: this notebook will run with the following model IDs :\n",
    "- `meta-textgeneration-llama-2-7b-f`\n",
    "- `meta-textgeneration-llama-2-13b-f`\n",
    "- `meta-textgeneration-llama-2-70b-f`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08909d09",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgeneration-llama-2-7b-f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3f923bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version = \"3.2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eef0dd",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart. For successful deployment, you must manually change the `accept_eula` argument in the model's deploy method to `True`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version, instance_type=\"ml.g5.2xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7499e62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-f-2024-07-30-10-29-46-017\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-f-2024-07-30-10-29-51-544\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-f-2024-07-30-10-29-51-544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "***\n",
    "### Supported Parameters\n",
    "\n",
    "***\n",
    "This model supports many parameters while performing inference. They include:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches `max_new_tokens`. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must be a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments.\n",
    "\n",
    "**NOTE**: If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "\n",
    "**NOTE**: This model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9e9e",
   "metadata": {},
   "source": [
    "### Example prompts\n",
    "***\n",
    "The examples in this section demonstrate how to perform text generation with conversational dialog as prompt inputs. Example payloads are retrieved programmatically from the `JumpStartModel` object.\n",
    "\n",
    "Input messages for Llama-2 chat models should exhibit the following format. The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...). The last message must be from 'user'. A simple user prompt may look like the following:\n",
    "```\n",
    "<s>[INST] {user_prompt} [/INST]\n",
    "```\n",
    "You may also add a system prompt with the following syntax:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_prompt} [/INST]\n",
    "```\n",
    "Finally, you can have a conversational interaction with the model by including all previous user prompts and assistant responses in the input:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_prompt_1} [/INST] {assistant_response_1} </s><s>[INST] {user_prompt_1} [/INST]\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b11b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example_payloads = model.retrieve_all_examples()\n",
    "\n",
    "# for payload in example_payloads[:1]:\n",
    "#     response = predictor.predict(payload.body)\n",
    "#     print(\"\\nInput\\n\", payload.body, \"\\n\\nOutput\\n\", response[0][\"generated_text\"], \"\\n\\n===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e024dd-2e12-4da3-a31b-30377da889fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c351c54-0c44-4661-9f45-e5a5020266b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.4)\n",
      "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-2.10.1 sacrebleu-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f7a28-e94a-49d7-a39d-bb49d9320e06",
   "metadata": {},
   "source": [
    "## bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2c8c0-a632-4e45-ae51-0998308b295d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacrebleu 2.4.2\n"
     ]
    }
   ],
   "source": [
    "!sacrebleu --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acab7e76-d67d-4c74-a7b8-f407e756f3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score (): 100.00000000000004\n",
      "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 35 ref_len = 35)\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# 参考文本（字符串）\n",
    "reference_text = \"The patient is referred due to the rapid progression of the tumor despite initial treatment modalities. Further evaluation and management from a specialized neuro-oncology team are essential for this recurrent and aggressive tumor.\"\n",
    "\n",
    "# 生成的文本（字符串）\n",
    "candidate_text = \"The patient is referred due to the rapid progression of the tumor despite initial treatment modalities. Further evaluation and management from a specialized neuro-oncology team are essential for this recurrent and aggressive tumor.\"\n",
    "\n",
    "# 计算BLEU分数，调整n-gram权重\n",
    "# 默认权重是(0.25, 0.25, 0.25, 0.25)，分别对应1-gram到4-gram\n",
    "# 我们可以调整权重，例如只考虑1-gram和2-gram，权重分别为(0.5, 0.5)\n",
    "# 设置n-gram权重\n",
    "weights = (0.5, 0.5, 0, 0)\n",
    "\n",
    "# 创建BLEU对象，并传入自定义的权重\n",
    "weights = (0.5, 0.5, 0, 0)\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "print(f\"BLEU score (): {bleu.score}\")\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ae564-194a-4b95-8b7b-ffcbbffec319",
   "metadata": {},
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "from sagemaker.jumpstart.model import JumpStartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc5de194-999d-4130-86dd-ff07124a4ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d70113-6594-4263-9a92-46c841d64f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores: {'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "810c2bf9-3e73-463c-98c2-9e417a267c91",
   "metadata": {},
   "source": [
    "# deploy llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "76c0b1d0-68c8-4eef-9cb3-c1361e5ba728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " Output:\n",
      "\n",
      "is_Papilledema: False\n",
      "referral_content: \"Further diagnostic evaluation and potential therapeutic intervention from a hepatologist are recommended to determine the nature of the liver mass and address the elevated hepatic transaminases.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"letter1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    letter = f.read()\n",
    "# print(letter)\n",
    "\n",
    "response = predictor.predict({'inputs': letter,\n",
    "                             'parameters': {'max_new_tokens': 128}})\n",
    "\n",
    "print(\"Output:\\n\", response[0][\"generated_text\"].strip(), end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00feeefc-ec8f-4343-9949-a76e35abe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lewis test all test.json with llama2 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ae68cf0-d6e3-477d-9619-34a017a15c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "def parse_model_response(response_text):\n",
    "    # Split the response into lines and look for the referral_content line\n",
    "    lines = response_text.split('\\n')\n",
    "    for line in lines:\n",
    "        if \"referral_content:\" in line:\n",
    "            return line.split(\"referral_content:\", 1)[1].strip().strip('\"')\n",
    "    return \"extract failure\"\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    bleu_score_list = []\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test.get(\"instruction\", \"\")\n",
    "        whole_letter = single_test.get(\"whole_letter\", \"\")\n",
    "        referral_content = single_test.get(\"referral_content\", \"\")\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 512}})\n",
    "        \n",
    "        reference_text = referral_content\n",
    "        candidate_text = \"extract failure\"\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            candidate_text = parse_model_response(tmp_str)\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Error processing ID: {single_test.get('id', 'unknown')}\")\n",
    "            logging.error(f\"Response: {response[0]['generated_text'].strip() if response else 'No response'}\")\n",
    "            logging.error(err)\n",
    "        finally:\n",
    "            evaluate_list.append(candidate_text)\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "    \n",
    "    with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "        for single_test in test_data_json:\n",
    "            f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "    logging.info(f\"Predicted data has been saved to {output_jsonl_path}.\")\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test.get(\"id\", \"\"),\n",
    "            \"name\": single_test.get(\"name\", \"\"),\n",
    "            \"instruction\": single_test.get(\"instruction\", \"\"),\n",
    "            \"whole_letter\": single_test.get(\"whole_letter\", \"\"),\n",
    "            \"referral_content\": single_test.get(\"referral_content\", \"\"),\n",
    "            \"predict_referral_content\": single_test.get(\"predict_referral_content\", \"\"),\n",
    "            \"bleu\": single_test.get(\"bleu\", 0),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a26a7789-8ac5-4400-9291-82e412177914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicted data has been saved to 300724-4lewis-predict_llama2_7b_f_test.jsonl.\n",
      "INFO:root:CSV file has been saved to 300724-4lewis-predict_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"300724-4lewis-predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"300724-4lewis-predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2359f006-d935-4930-9409-8bea9bdc5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "def parse_model_response(response_text):\n",
    "    # Split the response into lines and look for the referral_content line\n",
    "    lines = response_text.split('\\n')\n",
    "    for line in lines:\n",
    "        if \"referral_content:\" in line:\n",
    "            return line.split(\"referral_content:\", 1)[1].strip().strip('\"')\n",
    "    return \"extract failure\"\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    bleu_score_list = []\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test.get(\"instruction\", \"\")\n",
    "        whole_letter = single_test.get(\"whole_letter\", \"\")\n",
    "        referral_content = single_test.get(\"referral_content\", \"\")\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 512}})\n",
    "        \n",
    "        reference_text = referral_content\n",
    "        candidate_text = \"extract failure\"\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            candidate_text = parse_model_response(tmp_str)\n",
    "            # Fallback mechanism to handle cases where primary extraction fails\n",
    "            if candidate_text == \"extract failure\":\n",
    "                if \"referral_content:\" in tmp_str:\n",
    "                    candidate_text = tmp_str.split(\"referral_content:\", 1)[1].strip().strip('\"')\n",
    "                elif \"The reason for this referral is\" in tmp_str:\n",
    "                    candidate_text = tmp_str.split(\"The reason for this referral is\", 1)[1].split(\".\")[0].strip()\n",
    "                elif \"The referral is being made because\" in tmp_str:\n",
    "                    candidate_text = tmp_str.split(\"The referral is being made because\", 1)[1].split(\".\")[0].strip()\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Error processing ID: {single_test.get('id', 'unknown')}\")\n",
    "            logging.error(f\"Response: {response[0]['generated_text'].strip() if response else 'No response'}\")\n",
    "            logging.error(err)\n",
    "        finally:\n",
    "            evaluate_list.append(candidate_text)\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "    \n",
    "    with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "        for single_test in test_data_json:\n",
    "            f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "    logging.info(f\"Predicted data has been saved to {output_jsonl_path}.\")\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test.get(\"id\", \"\"),\n",
    "            \"name\": single_test.get(\"name\", \"\"),\n",
    "            \"instruction\": single_test.get(\"instruction\", \"\"),\n",
    "            \"whole_letter\": single_test.get(\"whole_letter\", \"\"),\n",
    "            \"referral_content\": single_test.get(\"referral_content\", \"\"),\n",
    "            \"predict_referral_content\": single_test.get(\"predict_referral_content\", \"\"),\n",
    "            \"bleu\": single_test.get(\"bleu\", 0),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bc33783e-2daa-4cce-b82d-c8ebbf91d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicted data has been saved to 300724-5lewis-predict_llama2_7b_f_test.jsonl.\n",
      "INFO:root:CSV file has been saved to 300724-5lewis-predict_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"300724-5lewis-predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"300724-5lewis-predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c45420a6-37ac-4528-b914-74c9def162d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "def parse_model_response(response_text):\n",
    "    # Split the response into lines and look for the referral_content line\n",
    "    lines = response_text.split('\\n')\n",
    "    for line in lines:\n",
    "        if \"referral_content:\" in line:\n",
    "            return line.split(\"referral_content:\", 1)[1].strip().strip('\"')\n",
    "    return \"extract failure\"\n",
    "\n",
    "def extract_referral_content(tmp_str):\n",
    "    # Primary extraction using common patterns\n",
    "    patterns = [\n",
    "        r'referral_content\\s*:\\s*\"(.*?)\"',\n",
    "        r'The reason for this referral is\\s*(.*?)\\.',\n",
    "        r'The referral is being made because\\s*(.*?)\\.',\n",
    "        r'Referral Reason:\\s*(.*?)\\.',\n",
    "        r'The reason for referral is\\s*(.*?)\\.'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, tmp_str)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "\n",
    "    # Fallback extraction logic\n",
    "    sentences = tmp_str.split('. ')\n",
    "    for sentence in sentences:\n",
    "        if 'referral' in sentence.lower():\n",
    "            return sentence.strip()\n",
    "    \n",
    "    return \"extract failure\"\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    bleu_score_list = []\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test.get(\"instruction\", \"\")\n",
    "        whole_letter = single_test.get(\"whole_letter\", \"\")\n",
    "        referral_content = single_test.get(\"referral_content\", \"\")\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 512}})\n",
    "        \n",
    "        reference_text = referral_content\n",
    "        candidate_text = \"extract failure\"\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            candidate_text = extract_referral_content(tmp_str)\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Error processing ID: {single_test.get('id', 'unknown')}\")\n",
    "            logging.error(f\"Response: {response[0]['generated_text'].strip() if response else 'No response'}\")\n",
    "            logging.error(err)\n",
    "        finally:\n",
    "            evaluate_list.append(candidate_text)\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "    \n",
    "    with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "        for single_test in test_data_json:\n",
    "            f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "    logging.info(f\"Predicted data has been saved to {output_jsonl_path}.\")\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test.get(\"id\", \"\"),\n",
    "            \"name\": single_test.get(\"name\", \"\"),\n",
    "            \"instruction\": single_test.get(\"instruction\", \"\"),\n",
    "            \"whole_letter\": single_test.get(\"whole_letter\", \"\"),\n",
    "            \"referral_content\": single_test.get(\"referral_content\", \"\"),\n",
    "            \"predict_referral_content\": single_test.get(\"predict_referral_content\", \"\"),\n",
    "            \"bleu\": single_test.get(\"bleu\", 0),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b4fa30a-9741-4aff-81a5-92e41e25370a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicted data has been saved to 300724-6lewis-predict_llama2_7b_f_test.jsonl.\n",
      "INFO:root:CSV file has been saved to 300724-6lewis-predict_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"300724-6lewis-predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"300724-6lewis-predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eb9d1f38-4b87-41c3-9501-3425692ca581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "def parse_model_response(response_text):\n",
    "    # Split the response into lines and look for the referral_content line\n",
    "    lines = response_text.split('\\n')\n",
    "    for line in lines:\n",
    "        if \"referral_content:\" in line:\n",
    "            return line.split(\"referral_content:\", 1)[1].strip().strip('\"')\n",
    "    return \"extract failure\"\n",
    "\n",
    "def extract_referral_content(tmp_str):\n",
    "    # Primary extraction using common patterns\n",
    "    patterns = [\n",
    "        r'referral_content\\s*:\\s*\"(.*?)\"',\n",
    "        r'The reason for this referral is\\s*(.*?)\\.',\n",
    "        r'The referral is being made because\\s*(.*?)\\.',\n",
    "        r'Referral Reason:\\s*(.*?)\\.',\n",
    "        r'The reason for referral is\\s*(.*?)\\.'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, tmp_str)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "\n",
    "    # Fallback extraction logic\n",
    "    sentences = tmp_str.split('. ')\n",
    "    for sentence in sentences:\n",
    "        if 'referral' in sentence.lower():\n",
    "            return sentence.strip()\n",
    "    \n",
    "    return \"extract failure\"\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # Further clean and format the extracted text\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    bleu_score_list = []\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test.get(\"instruction\", \"\")\n",
    "        whole_letter = single_test.get(\"whole_letter\", \"\")\n",
    "        referral_content = single_test.get(\"referral_content\", \"\")\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 512}})\n",
    "        \n",
    "        reference_text = referral_content if referral_content is not None else \"\"\n",
    "        candidate_text = \"extract failure\"\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            candidate_text = extract_referral_content(tmp_str)\n",
    "            candidate_text = clean_extracted_text(candidate_text)\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Error processing ID: {single_test.get('id', 'unknown')}\")\n",
    "            logging.error(f\"Response: {response[0]['generated_text'].strip() if response else 'No response'}\")\n",
    "            logging.error(err)\n",
    "        finally:\n",
    "            evaluate_list.append(candidate_text)\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "    \n",
    "    with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "        for single_test in test_data_json:\n",
    "            f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "    logging.info(f\"Predicted data has been saved to {output_jsonl_path}.\")\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test.get(\"id\", \"\"),\n",
    "            \"name\": single_test.get(\"name\", \"\"),\n",
    "            \"instruction\": single_test.get(\"instruction\", \"\"),\n",
    "            \"whole_letter\": single_test.get(\"whole_letter\", \"\"),\n",
    "            \"referral_content\": single_test.get(\"referral_content\", \"\"),\n",
    "            \"predict_referral_content\": single_test.get(\"predict_referral_content\", \"\"),\n",
    "            \"bleu\": single_test.get(\"bleu\", 0),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c091d397-3f5c-4c52-8e9a-83a5aefddec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicted data has been saved to 300724-7lewis-predict_llama2_7b_f_test.jsonl.\n",
      "INFO:root:CSV file has been saved to 300724-7lewis-predict_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"300724-7lewis-predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"300724-7lewis-predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "386d0f4b-2675-43c3-b55a-f7986295b70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分数大于100的个数：46, 占所有数据的百分比为： 0.32857142857142857\n",
      "分数大于70的个数：56, 占所有数据的百分比为： 0.4\n",
      "bleu平均分数: 50.374449038232484\n"
     ]
    }
   ],
   "source": [
    "analyze_predict_data(test_bleu_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "64b333ed-c4f8-4502-85a7-b8e936aa8973",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "def extract_referral_content(tmp_str):\n",
    "    # Refined primary extraction using common patterns\n",
    "    patterns = [\n",
    "        r'referral_content\\s*:\\s*\"(.*?)\"',\n",
    "        r'The reason for this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The referral is being made because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Referral Reason:\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The reason for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'need referral because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'This referral is made to\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'We are referring this patient because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The purpose of this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Our reason for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Due to\\s*(.*?)(?<!\\s)\\.',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, tmp_str)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "\n",
    "    # Fallback extraction logic\n",
    "    sentences = tmp_str.split('. ')\n",
    "    for sentence in sentences:\n",
    "        if 'referral' in sentence.lower():\n",
    "            return sentence.strip()\n",
    "    \n",
    "    return \"extract failure\"\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # Further clean and format the extracted text\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    bleu_score_list = []\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test.get(\"instruction\", \"\")\n",
    "        whole_letter = single_test.get(\"whole_letter\", \"\")\n",
    "        referral_content = single_test.get(\"referral_content\", \"\")\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 512}})\n",
    "        \n",
    "        reference_text = referral_content if referral_content is not None else \"\"\n",
    "        candidate_text = \"extract failure\"\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            candidate_text = extract_referral_content(tmp_str)\n",
    "            candidate_text = clean_extracted_text(candidate_text)\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Error processing ID: {single_test.get('id', 'unknown')}\")\n",
    "            logging.error(f\"Response: {response[0]['generated_text'].strip() if response else 'No response'}\")\n",
    "            logging.error(err)\n",
    "        finally:\n",
    "            evaluate_list.append(candidate_text)\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "    \n",
    "    with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "        for single_test in test_data_json:\n",
    "            f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "    logging.info(f\"Predicted data has been saved to {output_jsonl_path}.\")\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test.get(\"id\", \"\"),\n",
    "            \"name\": single_test.get(\"name\", \"\"),\n",
    "            \"instruction\": single_test.get(\"instruction\", \"\"),\n",
    "            \"whole_letter\": single_test.get(\"whole_letter\", \"\"),\n",
    "            \"referral_content\": single_test.get(\"referral_content\", \"\"),\n",
    "            \"predict_referral_content\": single_test.get(\"predict_referral_content\", \"\"),\n",
    "            \"bleu\": single_test.get(\"bleu\", 0),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c57c3544-edce-44ef-9e57-348edb8866bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicted data has been saved to 300724-8lewis-predict_llama2_7b_f_test.jsonl.\n",
      "INFO:root:CSV file has been saved to 300724-8lewis-predict_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"300724-8lewis-predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"300724-8lewis-predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "34ad30b9-260e-4bf7-8b23-2d913df93c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "def replace_output_prefix(input_str):\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "def extract_referral_content(tmp_str):\n",
    "    # Refined primary extraction using common patterns\n",
    "    patterns = [\n",
    "        r'referral_content\\s*:\\s*\"(.*?)\"',\n",
    "        r'The reason for this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The referral is being made because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Referral Reason:\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The reason for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'need referral because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'This referral is made to\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'We are referring this patient because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The purpose of this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Our reason for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Due to\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'because of\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'As a result of\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The intent of this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, tmp_str)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "\n",
    "    # Fallback extraction logic\n",
    "    sentences = tmp_str.split('. ')\n",
    "    for sentence in sentences:\n",
    "        if 'referral' in sentence.lower():\n",
    "            return sentence.strip()\n",
    "    \n",
    "    return \"extract failure\"\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # Further clean and format the extracted text\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    bleu_score_list = []\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test.get(\"instruction\", \"\")\n",
    "        whole_letter = single_test.get(\"whole_letter\", \"\")\n",
    "        referral_content = single_test.get(\"referral_content\", \"\")\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 512}})\n",
    "        \n",
    "        reference_text = referral_content if referral_content is not None else \"\"\n",
    "        candidate_text = \"extract failure\"\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            candidate_text = extract_referral_content(tmp_str)\n",
    "            candidate_text = clean_extracted_text(candidate_text)\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Error processing ID: {single_test.get('id', 'unknown')}\")\n",
    "            logging.error(f\"Response: {response[0]['generated_text'].strip() if response else 'No response'}\")\n",
    "            logging.error(err)\n",
    "        finally:\n",
    "            evaluate_list.append(candidate_text)\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "    \n",
    "    with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "        for single_test in test_data_json:\n",
    "            f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "    logging.info(f\"Predicted data has been saved to {output_jsonl_path}.\")\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test.get(\"id\", \"\"),\n",
    "            \"name\": single_test.get(\"name\", \"\"),\n",
    "            \"instruction\": single_test.get(\"instruction\", \"\"),\n",
    "            \"whole_letter\": single_test.get(\"whole_letter\", \"\"),\n",
    "            \"referral_content\": single_test.get(\"referral_content\", \"\"),\n",
    "            \"predict_referral_content\": single_test.get(\"predict_referral_content\", \"\"),\n",
    "            \"bleu\": single_test.get(\"bleu\", 0),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "64c6d503-37f6-499a-9169-db565572c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicted data has been saved to 300724-9lewis-predict_llama2_7b_f_test.jsonl.\n",
      "INFO:root:CSV file has been saved to 300724-9lewis-predict_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"300724-9lewis-predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"300724-9lewis-predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "76f7f577-7a99-4598-b522-69621adeeb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "def extract_referral_content(tmp_str):\n",
    "    # Refined primary extraction using common patterns\n",
    "    patterns = [\n",
    "        r'referral_content\\s*:\\s*\"(.*?)\"',\n",
    "        r'The reason for this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The referral is being made because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Referral Reason:\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The reason for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'need referral because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'This referral is made to\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'We are referring this patient because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The purpose of this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Our reason for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Due to\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'because of\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'As a result of\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The intent of this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        # Specific patterns for provided examples\n",
    "        r'Given the imaging and laboratory findings,\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Given the persistent and progressive nature of the swelling,\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Mr. Doe requires a specialist evaluation for\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'Given the complexity and progression of her condition,\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'The imaging studies confirm the diagnosis of\\s*(.*?)(?<!\\s)\\.',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, tmp_str)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "\n",
    "    # Fallback extraction logic\n",
    "    sentences = tmp_str.split('. ')\n",
    "    for sentence in sentences:\n",
    "        if 'referral' in sentence.lower():\n",
    "            return sentence.strip()\n",
    "    \n",
    "    return \"extract failure\"\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # Further clean and format the extracted text\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    bleu_score_list = []\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test.get(\"instruction\", \"\")\n",
    "        whole_letter = single_test.get(\"whole_letter\", \"\")\n",
    "        referral_content = single_test.get(\"referral_content\", \"\")\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 512}})\n",
    "        \n",
    "        reference_text = referral_content if referral_content is not None else \"\"\n",
    "        candidate_text = \"extract failure\"\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            candidate_text = extract_referral_content(tmp_str)\n",
    "            candidate_text = clean_extracted_text(candidate_text)\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Error processing ID: {single_test.get('id', 'unknown')}\")\n",
    "            logging.error(f\"Response: {response[0]['generated_text'].strip() if response else 'No response'}\")\n",
    "            logging.error(err)\n",
    "        finally:\n",
    "            evaluate_list.append(candidate_text)\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "    \n",
    "    with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "        for single_test in test_data_json:\n",
    "            f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "    logging.info(f\"Predicted data has been saved to {output_jsonl_path}.\")\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test.get(\"id\", \"\"),\n",
    "            \"name\": single_test.get(\"name\", \"\"),\n",
    "            \"instruction\": single_test.get(\"instruction\", \"\"),\n",
    "            \"whole_letter\": single_test.get(\"whole_letter\", \"\"),\n",
    "            \"referral_content\": single_test.get(\"referral_content\", \"\"),\n",
    "            \"predict_referral_content\": single_test.get(\"predict_referral_content\", \"\"),\n",
    "            \"bleu\": single_test.get(\"bleu\", 0),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc69885d-6f29-499b-b2af-36e9818a5697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicted data has been saved to 300724-10lewis-predict_llama2_7b_f_test.jsonl.\n",
      "INFO:root:CSV file has been saved to 300724-10lewis-predict_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"300724-10lewis-predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"300724-10lewis-predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "37dcb85b-fb20-475b-9766-e4898c9cd4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分数大于100的个数：46, 占所有数据的百分比为： 0.32857142857142857\n",
      "分数大于70的个数：56, 占所有数据的百分比为： 0.4\n",
      "bleu平均分数: 50.374449038232484\n"
     ]
    }
   ],
   "source": [
    "analyze_predict_data(test_bleu_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61713c51-7860-46be-86d8-71f10c37b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL TEST SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1d986dc1-46af-470c-baed-c9bcfd383732",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "def extract_referral_content(tmp_str):\n",
    "    # Refined primary extraction using common patterns\n",
    "    patterns = [\n",
    "        r'\\s*referral_content\\s*:\\s*\"(.*?)\"',\n",
    "        r'\\s*The reason for this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*The referral is being made because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*Referral Reason:\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*The reason for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*need referral because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*This referral is made to\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*We are referring this patient because\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*The purpose of this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*Our reason for referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*Due to\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*because of\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*As a result of\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*The intent of this referral is\\s*(.*?)(?<!\\s)\\.',\n",
    "        # Specific patterns for provided examples\n",
    "        r'\\s*Given the imaging and laboratory findings,\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*Given the persistent and progressive nature of the swelling,\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*Mr. Doe requires a specialist evaluation for\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*Given the complexity and progression of her condition,\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*The imaging studies confirm the diagnosis of\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*He would benefit from\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*Mr. Doe requires\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*specialist evaluation for\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*comprehensive evaluation and appropriate management by\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*to determine the nature of\\s*(.*?)(?<!\\s)\\.',\n",
    "        r'\\s*Given its persistence and unresponsiveness to initial therapies,\\s*(.*?)(?<!\\s)\\.',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, tmp_str)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "\n",
    "    # Fallback extraction logic\n",
    "    sentences = tmp_str.split('. ')\n",
    "    for sentence in sentences:\n",
    "        if 'referral' in sentence.lower():\n",
    "            return sentence.strip()\n",
    "    \n",
    "    return \"extract failure\"\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # Further clean and format the extracted text\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    bleu_score_list = []\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test.get(\"instruction\", \"\")\n",
    "        whole_letter = single_test.get(\"whole_letter\", \"\")\n",
    "        referral_content = single_test.get(\"referral_content\", \"\")\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 512}})\n",
    "        \n",
    "        reference_text = referral_content if referral_content is not None else \"\"\n",
    "        candidate_text = \"extract failure\"\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            candidate_text = extract_referral_content(tmp_str)\n",
    "            candidate_text = clean_extracted_text(candidate_text)\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Error processing ID: {single_test.get('id', 'unknown')}\")\n",
    "            logging.error(f\"Response: {response[0]['generated_text'].strip() if response else 'No response'}\")\n",
    "            logging.error(err)\n",
    "        finally:\n",
    "            evaluate_list.append(candidate_text)\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "    \n",
    "    with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "        for single_test in test_data_json:\n",
    "            f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "    logging.info(f\"Predicted data has been saved to {output_jsonl_path}.\")\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test.get(\"id\", \"\"),\n",
    "            \"name\": single_test.get(\"name\", \"\"),\n",
    "            \"instruction\": single_test.get(\"instruction\", \"\"),\n",
    "            \"whole_letter\": single_test.get(\"whole_letter\", \"\"),\n",
    "            \"referral_content\": single_test.get(\"referral_content\", \"\"),\n",
    "            \"predict_referral_content\": single_test.get(\"predict_referral_content\", \"\"),\n",
    "            \"bleu\": single_test.get(\"bleu\", 0),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8950a310-567f-4b66-aff9-877713f6b4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicted data has been saved to 300724-finallewis-predict_llama2_7b_f_test.jsonl.\n",
      "INFO:root:CSV file has been saved to 300724-finallewis-predict_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"300724-finallewis-predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"300724-finallewis-predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "505cf228-ea48-4dae-b690-a0d3ec32a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分数大于100的个数：46, 占所有数据的百分比为： 0.32857142857142857\n",
      "分数大于70的个数：56, 占所有数据的百分比为： 0.4\n",
      "bleu平均分数: 50.374449038232484\n"
     ]
    }
   ],
   "source": [
    "analyze_predict_data(test_bleu_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0091ce-548b-442c-8578-ff7f87834243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078121b-f575-4fa0-bee9-c189ca6e7bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4b74d-4f5e-4998-b1d7-e73f84a73c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f590b7c3-976d-4893-95eb-293c0a6ac0b9",
   "metadata": {},
   "source": [
    "## test all test.json with llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee85f4-15a1-419a-aab0-95abfcf13c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69ff2f49-ab87-472e-bb6d-db0dd3c05d39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "## Adjust function to handle and extract the referral_content from the simulated output \n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    # use regex to replace the prefix, ignoring case\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    \n",
    "    bleu_score_list = []\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test[\"instruction\"]\n",
    "        whole_letter = single_test[\"whole_letter\"]\n",
    "        referral_content = single_test[\"referral_content\"]\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt,\n",
    "                                 'parameters': {'max_new_tokens': 256}})\n",
    "        # print(prompt)\n",
    "        reference_text = referral_content\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            # Extract referral_content using regular expressions\n",
    "            match = re.search(r'referral_content:\\s*\"([^\"]+)\"', tmp_str)\n",
    "            if match:\n",
    "                candidate_text = match.group(1)\n",
    "            else:\n",
    "                candidate_text = \"extract failure\"\n",
    "        except Exception as err:\n",
    "            print(single_test[\"id\"])\n",
    "            print(response[0][\"generated_text\"].strip())\n",
    "            print()\n",
    "            candidate_text = \"extract failure\"\n",
    "        finally:\n",
    "\n",
    "            evaluate_list.append(candidate_text)\n",
    "            # print(\"predict: \" + candidate_text)\n",
    "            # print(\"real: \" + reference_text)\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            # print(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "            \n",
    "    with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "         for single_test in test_data_json:\n",
    "             f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "    print(f\"predicted data has been saved to {output_path}.\")\n",
    "    \n",
    "    # Create and save CSV file\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test[\"id\"],\n",
    "            \"name\": single_test[\"name\"],\n",
    "            \"instruction\": single_test[\"instruction\"],\n",
    "            \"whole_letter\": single_test[\"whole_letter\"],\n",
    "            \"referral_content\": single_test[\"referral_content\"],\n",
    "            \"predict_referral_content\": single_test[\"predict_referral_content\"],\n",
    "            \"bleu\": single_test[\"bleu\"],\n",
    "        })\n",
    "\n",
    "    # create DataFrame\n",
    "    df = pd.DataFrame(csv_data)\n",
    "\n",
    "    # save to csv file\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e9107b4-f214-42d0-9f50-e1e3f154b9cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_evaluate_list, test_bleu_score_list \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_jsonl_with_llama2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_data_735/test.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m290724lewis-predict_llama2_7b_f_test.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m290724lewis-predict_llama2_7b_f_test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m, in \u001b[0;36mevaluate_jsonl_with_llama2\u001b[0;34m(predictor, path, output_jsonl_path, csv_file_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m referral_content \u001b[38;5;241m=\u001b[39m single_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferral_content\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m###\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mwhole_letter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m###\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print(prompt)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m reference_text \u001b[38;5;241m=\u001b[39m referral_content\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/base_predictor.py:212\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes, component_name)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inference_component_name:\n\u001b[1;32m    210\u001b[0m     request_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferenceComponentName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m inference_component_name\n\u001b[0;32m--> 212\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:999\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    995\u001b[0m     maybe_compress_request(\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[1;32m    997\u001b[0m     )\n\u001b[1;32m    998\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m--> 999\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1005\u001b[0m     http_response\u001b[38;5;241m=\u001b[39mhttp,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1009\u001b[0m )\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1027\u001b[0m             exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   1028\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1029\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/endpoint.py:197\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m    196\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[0;32m--> 197\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[1;32m    201\u001b[0m     attempts,\n\u001b[1;32m    202\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     exception,\n\u001b[1;32m    206\u001b[0m ):\n\u001b[1;32m    207\u001b[0m     attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/endpoint.py:239\u001b[0m, in \u001b[0;36mEndpoint._get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, operation_model, context):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# This will return a tuple of (success_response, exception)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# and success_response is itself a tuple of\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# (http_response, parsed_dict).\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# If an exception occurs then the success_response is None.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# If no exception occurs then exception is None.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     kwargs_to_emit \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_response\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: context,\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m'\u001b[39m: exception,\n\u001b[1;32m    247\u001b[0m     }\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/endpoint.py:279\u001b[0m, in \u001b[0;36mEndpoint._do_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    277\u001b[0m     http_response \u001b[38;5;241m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m http_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         http_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/endpoint.py:375\u001b[0m, in \u001b[0;36mEndpoint._send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/httpsession.py:464\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    461\u001b[0m     conn\u001b[38;5;241m.\u001b[39mproxy_headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m host\n\u001b[1;32m    463\u001b[0m request_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_target(request\u001b[38;5;241m.\u001b[39murl, proxy_url)\n\u001b[0;32m--> 464\u001b[0m urllib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m http_response \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mawsrequest\u001b[38;5;241m.\u001b[39mAWSResponse(\n\u001b[1;32m    477\u001b[0m     request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    478\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    479\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    480\u001b[0m     urllib_response,\n\u001b[1;32m    481\u001b[0m )\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m request\u001b[38;5;241m.\u001b[39mstream_output:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# Cause the raw stream to be exhausted immediately. We do it\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# this way instead of using preload_content because\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# preload_content will never buffer chunked responses\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"test_data_735/test.jsonl\", \"290724lewis-predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"290724lewis-predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf805980-8c39-4148-8e34-03c254e6cfba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been saved to ../train_test_data/predict_llama2_7b_f_train.csv\n"
     ]
    }
   ],
   "source": [
    "train_evaluate_list, train_bleu_score_list,train_rouge_score_list = evaluate_jsonl_with_llama2(predictor,\n",
    "                                                                \"../train_test_data/train.jsonl\", \"../train_test_data/predict_llama2_7b_f_train.jsonl\",\n",
    "                                                                \"../train_test_data/predict_llama2_7b_f_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "464dddd9-55c2-423f-b53f-3b941bc3ca67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_predict_data(bleu_score_list):\n",
    "    # 统计大于100的个数\n",
    "    count_gt_100 = sum(1 for score in bleu_score_list if score >= 100)\n",
    "\n",
    "    # 统计大于70的个数\n",
    "    count_gt_70 = sum(1 for score in bleu_score_list if score > 70)\n",
    "\n",
    "    prob_gt_100 = count_gt_100 / len(bleu_score_list)\n",
    "    prob_gt_70 = count_gt_70 / len(bleu_score_list)\n",
    "    average_score = sum(bleu_score_list) / float(len(bleu_score_list))\n",
    "\n",
    "    print(f\"分数大于100的个数：{count_gt_100}, 占所有数据的百分比为： {prob_gt_100}\")\n",
    "    print(f\"分数大于70的个数：{count_gt_70}, 占所有数据的百分比为： {prob_gt_70}\")\n",
    "    print(f\"bleu平均分数: {average_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95ca531f-9b93-4ae8-8c63-e523cb200e06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分数大于100的个数：46, 占所有数据的百分比为： 0.32857142857142857\n",
      "分数大于70的个数：56, 占所有数据的百分比为： 0.4\n",
      "bleu平均分数: 50.374449038232484\n"
     ]
    }
   ],
   "source": [
    "analyze_predict_data(test_bleu_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f96f918-a7ef-49a4-9fb6-98d1cdf58583",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分数大于100的个数：10, 占所有数据的百分比为： 0.47619047619047616\n",
      "分数大于70的个数：11, 占所有数据的百分比为： 0.5238095238095238\n",
      "bleu平均分数: 62.56950311883414\n"
     ]
    }
   ],
   "source": [
    "analyze_predict_data(test_bleu_score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558fea8",
   "metadata": {},
   "source": [
    "***\n",
    "While not used in the previously provided example payloads, you can format your own messages to the Llama-2 model with the following utility function.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5adf9b4-c7e1-4090-aefe-9cae0d096968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format messages for Llama-2 chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join([\"<<SYS>>\\n\", messages[0][\"content\"], \"\\n<</SYS>>\\n\\n\", messages[1][\"content\"]])\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend([\"<s>\", \"[INST] \", (user[\"content\"]).strip(), \" [/INST] \", (answer[\"content\"]).strip(), \"</s>\"])\n",
    "\n",
    "    prompt.extend([\"<s>\", \"[INST] \", (messages[-1][\"content\"]).strip(), \" [/INST] \"])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "dialog = [\n",
    "    {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n",
    "    {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "]\n",
    "\n",
    "prompt = format_messages(dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24cc5560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c6e59-2172-4349-9c29-20b5015bde7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
