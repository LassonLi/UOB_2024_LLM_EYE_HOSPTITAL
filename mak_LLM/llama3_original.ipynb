{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbeb0ff-ef58-4a17-a9ea-37613aee6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to SageMaker JumpStart - Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e567a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e36b9",
   "metadata": {},
   "source": [
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy a SageMaker JumpStart text generation model and invoke the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af77efb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, upgrade to the latest sagemaker SDK to ensure all available models are deployable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b05b931-992e-4526-978d-f03196874a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb6b35",
   "metadata": {},
   "source": [
    "Select the desired model to deploy. The provided dropdown filters all text generation models available in SageMaker JumpStart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f625a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a40df34",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "accept_eula = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a0388",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Create a `JumpStartModel` object, which initializes default model configurations conditioned on the selected instance type. JumpStart already sets a default instance type, but you can deploy the model on other instance types by passing `instance_type` to the `JumpStartModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a2a8e5-789f-4041-9927-221257126653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'meta-textgeneration-llama-3-8b-instruct' with wildcard version identifier '*'. You can pin to version '2.2.1' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259ad4f",
   "metadata": {},
   "source": [
    "You can now deploy the model using SageMaker JumpStart. If the selected model is gated, you will need to accept the end-user license agreement (EULA) prior to deployment. This is accomplished by providing the `accept_eula=True` argument to the `deploy` method. The deployment might take few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3b42ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(accept_eula=accept_eula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10cf06c",
   "metadata": {},
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "This section demonstrates how to invoke the endpoint using example payloads that are retrieved programmatically from the `JumpStartModel` object. You can replace these example payloads with your own payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_payloads = model.retrieve_all_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf5899c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " {'inputs': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat is the recipe of mayonnaise?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6, 'details': True, 'stop': '<|eot_id|>'}}\n",
      "\n",
      "Output:\n",
      " The classic condiment! Mayonnaise is a thick, creamy sauce made from a combination of oil, egg yolks, acid (such as vinegar or lemon juice), and seasonings. Here's a simple recipe to make mayonnaise at home:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 2 egg yolks\n",
      "* 1 tablespoon lemon juice or vinegar (such as apple cider vinegar or white wine vinegar)\n",
      "* 1/2 teaspoon Dijon mustard (optional, but recommended for flavor)\n",
      "* 1/2 cup (120 ml) neutral-tasting oil, such as canola, grapeseed, or sunflower oil\n",
      "* Salt, to taste\n",
      "* Water, as needed\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Start with room temperature ingredients**: Make sure your egg yolks, lemon juice, and oil are at room temperature. This will help the mixture emulsify smoothly.\n",
      "2. **Whisk the egg yolks**: In a medium-sized bowl, whisk together the egg yolks and lemon juice until well combined.\n",
      "3. **Add the mustard (if using)**: Whisk in the Dijon mustard, if using.\n",
      "4. **Slowly add the oil**: While continuously whisking the egg yolk mixture, slowly pour in the oil\n",
      "\n",
      "\n",
      "Input:\n",
      " {'inputs': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nI am going to Paris, what should I see?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is so great about #1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6, 'stop': '<|eot_id|>'}}\n",
      "\n",
      "Output:\n",
      " The Eiffel Tower! It's an iconic symbol of Paris and one of the most recognizable landmarks in the world. Here are some reasons why it's so great:\n",
      "\n",
      "1. **Engineering marvel**: The Eiffel Tower was the tallest structure in the world when it was built for the 1889 World's Fair, and it's still an impressive feat of engineering. It's made of over 18,000 pieces of wrought iron and stands 324 meters (1,063 feet) tall.\n",
      "2. **Panoramic views**: The Eiffel Tower offers breathtaking views of the city from its observation decks, which are 57 meters (187 feet) and 115 meters (377 feet) above ground. On a clear day, you can see for miles in every direction.\n",
      "3. **Romantic atmosphere**: The Eiffel Tower is often associated with romance, and it's a popular spot for couples to propose, get married, or simply enjoy a romantic dinner at one of the many restaurants with a view.\n",
      "4. **History**: The Eiffel Tower has played a significant role in French history, serving as a symbol of French culture and engineering prowess. It's also been the site of many famous events, including the first radio broadcast and the first\n",
      "\n",
      "\n",
      "Input:\n",
      " {'inputs': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nAlways answer with Haiku<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI am going to Paris, what should I see?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6, 'stop': '<|eot_id|>'}}\n",
      "\n",
      "Output:\n",
      " Eiffel Tower high\n",
      "River Seine's gentle whispers\n",
      "Art and love abound\n",
      "\n",
      "\n",
      "Input:\n",
      " {'inputs': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nAlways answer with emojis<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow to go from Beijing to NY?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6, 'stop': '<|eot_id|>'}}\n",
      "\n",
      "Output:\n",
      " 🛫️🗼️🛬\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for payload in example_payloads:\n",
    "    response = predictor.predict(payload.body)\n",
    "    response = response[0] if isinstance(response, list) else response\n",
    "    print(\"Input:\\n\", payload.body, end=\"\\n\\n\")\n",
    "    print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd54758-3149-431f-a5f8-2a744de9f8a4",
   "metadata": {},
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e9725b-bcd4-4524-b91b-6c31acbc04dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.4)\n",
      "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-2.10.1 sacrebleu-2.4.2\n",
      "sacrebleu 2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk sacrebleu\n",
    "!sacrebleu --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f27196dd-255b-436f-8388-f5d0c336a2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score (): 100.00000000000004\n",
      "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 35 ref_len = 35)\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# 参考文本（字符串）\n",
    "reference_text = \"The patient is referred due to the rapid progression of the tumor despite initial treatment modalities. Further evaluation and management from a specialized neuro-oncology team are essential for this recurrent and aggressive tumor.\"\n",
    "\n",
    "# 生成的文本（字符串）\n",
    "candidate_text = \"The patient is referred due to the rapid progression of the tumor despite initial treatment modalities. Further evaluation and management from a specialized neuro-oncology team are essential for this recurrent and aggressive tumor.\"\n",
    "\n",
    "# 计算BLEU分数，调整n-gram权重\n",
    "# 默认权重是(0.25, 0.25, 0.25, 0.25)，分别对应1-gram到4-gram\n",
    "# 我们可以调整权重，例如只考虑1-gram和2-gram，权重分别为(0.5, 0.5)\n",
    "# 设置n-gram权重\n",
    "weights = (0.5, 0.5, 0, 0)\n",
    "\n",
    "# 创建BLEU对象，并传入自定义的权重\n",
    "weights = (0.5, 0.5, 0, 0)\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "print(f\"BLEU score (): {bleu.score}\")\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be65b3e-6354-45f7-849e-d277ae77fbbb",
   "metadata": {},
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc4108c7-4bab-4bda-9184-9cbbfc1cff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22fe5af6-41a4-4b8c-97d0-469c19fece6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores: {'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# 计算ROUGE分数\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(candidate_text, reference_text, avg=True) # 由于只有一个 reference，所以 avg没有影响\n",
    "print(f\"ROUGE scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c07884a-5b0f-4821-9e90-73b1404b0ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " Output:\n",
      "{\n",
      "\"is_Papilledema\": False,\n",
      "\"referral_content\": \"Kindly see him for a specialist evaluation to determine the best course of action. I believe a more intensive investigation, including possibly imaging and more advanced pulmonary function tests, might be necessary.\"\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"letter.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    letter = f.read()\n",
    "# print(letter)\n",
    "\n",
    "response = predictor.predict({'inputs': letter,\n",
    "                             'parameters': {'max_new_tokens': 128}})\n",
    "response = response[0] if isinstance(response, list) else response\n",
    "\n",
    "print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffbba033-649a-4c86-a611-5c625b55b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test all test.json with llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78baba8-ff22-49a6-b561-9ba7aa58729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from rouge import Rouge\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    # 使用正则表达式进行替换，忽略大小写\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_braced_content(input_str):\n",
    "    # 使用正则表达式提取大括号及其内的内容\n",
    "    match = re.search(r'\\{[^{}]*\\}', input_str)\n",
    "    if match:\n",
    "        # 返回包含大括号及其内容的匹配部分\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        # 如果没有匹配到内容，返回空字符串或其他标记\n",
    "        return ''\n",
    "\n",
    "\n",
    "def extract_referral_content(data_str):\n",
    "    # 使用正则表达式提取referral_content的内容\n",
    "    match = re.search(r'\"referral_content\":\\s*(null|\".*?\")', data_str)\n",
    "    if match:\n",
    "        content = match.group(1)\n",
    "        if content == \"null\":\n",
    "            return \"\"\n",
    "        else:\n",
    "            # 去掉引号\n",
    "            return content.strip('\"')\n",
    "    return ''\n",
    "\n",
    "\n",
    "def evaluate_jsonl_with_llama3(predictor, path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    rouge_score_list = []\n",
    "    bleu_score_list = []\n",
    "\n",
    "    rouge = Rouge()\n",
    "\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test[\"instruction\"]\n",
    "        whole_letter = single_test[\"whole_letter\"]\n",
    "        referral_content = single_test[\"referral_content\"]\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt, 'parameters': {'max_new_tokens': 128}})\n",
    "        # print(prompt)\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "\n",
    "        response = response[0] if isinstance(response, list) else response\n",
    "        # print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")\n",
    "\n",
    "        reference_text = referral_content\n",
    "        # try:\n",
    "        tmp_str = replace_output_prefix(response[\"generated_text\"].strip())\n",
    "        print(tmp_str)\n",
    "#             tmp_str = extract_braced_content(tmp_str)\n",
    "#             # print(tmp_str)\n",
    "#             tmp = extract_referral_content(tmp_str)\n",
    "#             candidate_text = tmp\n",
    "#         except Exception as err:\n",
    "#             # print(single_test[\"id\"])\n",
    "#             # print(response[\"generated_text\"].strip())\n",
    "#             # print()\n",
    "#             candidate_text = \"extract failure\"\n",
    "#         finally:\n",
    "\n",
    "#             evaluate_list.append(candidate_text)\n",
    "#             print(\"predict: \" + candidate_text)\n",
    "#             print(\"real: \" + reference_text)\n",
    "\n",
    "\n",
    "#             bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "#             bleu_score_list.append(bleu.score)\n",
    "#             print(bleu.score)\n",
    "#             single_test[\"bleu\"] = bleu.score\n",
    "#             single_test[\"predict_referral_content\"] = candidate_text\n",
    "#             print()\n",
    "#             # 计算ROUGE分数\n",
    "#             # scores = rouge.get_scores(candidate_text, reference_text) # 由于只有一个 reference，所以 avg没有影响\n",
    "#             # rouge_score_list.append(scores)\n",
    "            \n",
    "# #     with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "# #         for single_test in test_data_json:\n",
    "# #             f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# #     print(f\"predicted data has been saved to {output_path}.\")\n",
    "    \n",
    "#     # 创建 CSV 文件\n",
    "#     csv_data = []\n",
    "\n",
    "#     for single_test in test_data_json:\n",
    "#         csv_data.append({\n",
    "#             # \"id\": single_test[\"id\"],\n",
    "#             # \"name\": single_test[\"name\"],\n",
    "#             \"instruction\": single_test[\"instruction\"],\n",
    "#             \"whole_letter\": single_test[\"whole_letter\"],\n",
    "#             \"referral_content\": single_test[\"referral_content\"],\n",
    "#             \"predict_referral_content\": single_test[\"predict_referral_content\"],\n",
    "#             \"bleu\": single_test[\"bleu\"],\n",
    "#         })\n",
    "\n",
    "#     # 创建 DataFrame\n",
    "#     df = pd.DataFrame(csv_data)\n",
    "\n",
    "#     # 保存为 CSV 文件\n",
    "#     df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "#     print(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "#     return evaluate_list, bleu_score_list, rouge_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7cb857-02cd-4f66-8d5d-51c448d0e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluate_list, test_bleu_score_list, test_rouge_score_list = evaluate_jsonl_with_llama3(predictor, \"test_data_735/test.jsonl\", \"llama3_original_test_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a53e57",
   "metadata": {},
   "source": [
    "## Clean up the endpoint\n",
    "Don't forget to clean up resources when finished to avoid unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35117832-710d-4308-91f0-a33da723a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predict_data(bleu_score_list):\n",
    "    # 统计大于100的个数\n",
    "    count_gt_100 = sum(1 for score in bleu_score_list if score >= 100)\n",
    "\n",
    "    # 统计大于70的个数\n",
    "    count_gt_70 = sum(1 for score in bleu_score_list if score > 70)\n",
    "\n",
    "    prob_gt_100 = count_gt_100 / len(bleu_score_list)\n",
    "    prob_gt_70 = count_gt_70 / len(bleu_score_list)\n",
    "    average_score = sum(bleu_score_list) / float(len(bleu_score_list))\n",
    "\n",
    "    print(f\"分数大于100的个数：{count_gt_100}, 占所有数据的百分比为： {prob_gt_100}\")\n",
    "    print(f\"分数大于70的个数：{count_gt_70}, 占所有数据的百分比为： {prob_gt_70}\")\n",
    "    print(f\"bleu平均分数: {average_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bb2ce-9e05-49d8-970b-5d3cfc41a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_predict_data(test_bleu_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4998b86",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|jumpstart-text-generation-inference.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
