{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# Run Llama 2 Models in SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b1b24",
   "metadata": {},
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy a JumpStart model for Text Generation using the Llama 2 fine-tuned model optimized for dialogue use cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095474d6-5b56-46c5-9ecf-78deaa80560a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully to ../dolly_dataset/train.jsonl\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "# 配置AWS凭证（如果需要）\n",
    "aws_access_key_id = 'AKIAVRUVRNSYBZUDMIDE'\n",
    "aws_secret_access_key = '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'\n",
    "region_name = 'eu-west-2'  # 例如 'us-west-2'\n",
    "\n",
    "# 创建S3客户端\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)\n",
    "\n",
    "def download_directory_from_s3(bucket_name, s3_prefix, local_dir):\n",
    "    \"\"\"\n",
    "    下载S3桶中的文件夹到本地目录\n",
    "    :param bucket_name: S3桶名称\n",
    "    :param s3_prefix: S3文件夹前缀\n",
    "    :param local_dir: 本地目录路径\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            local_file_path = os.path.join(local_dir, os.path.relpath(key, s3_prefix))\n",
    "            local_file_dir = os.path.dirname(local_file_path)\n",
    "\n",
    "            if not os.path.exists(local_file_dir):\n",
    "                os.makedirs(local_file_dir)\n",
    "\n",
    "            try:\n",
    "                s3.download_file(bucket_name, key, local_file_path)\n",
    "                print(f\"File downloaded successfully to {local_file_path}\")\n",
    "            except NoCredentialsError:\n",
    "                print(\"Credentials not available\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "\n",
    "# 使用示例\n",
    "bucket_name = 'shareddatasetllm'\n",
    "s3_prefix = 'dolly_dataset/'  # 例如 'data/folder/'\n",
    "local_dir = '../dolly_dataset/'  # 例如 '/home/user/folder/'\n",
    "\n",
    "download_directory_from_s3(bucket_name, s3_prefix, local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d458cf0-02e2-4066-927b-25fa5ef2a07e",
   "metadata": {},
   "source": [
    "***\n",
    "You can continue with the default model or choose a different model: this notebook will run with the following model IDs :\n",
    "- `meta-textgeneration-llama-2-7b-f`\n",
    "- `meta-textgeneration-llama-2-13b-f`\n",
    "- `meta-textgeneration-llama-2-70b-f`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08909d09",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgeneration-llama-2-7b-f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f923bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version = \"3.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eef0dd",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart. For successful deployment, you must manually change the `accept_eula` argument in the model's deploy method to `True`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using vulnerable JumpStart model 'meta-textgeneration-llama-2-7b-f' and version '3.2.0'.\n",
      "Using model 'meta-textgeneration-llama-2-7b-f' with wildcard version identifier '3.*'. You can pin to version '3.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version, instance_type=\"ml.g5.2xlarge\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7499e62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "***\n",
    "### Supported Parameters\n",
    "\n",
    "***\n",
    "This model supports many parameters while performing inference. They include:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches `max_new_tokens`. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must be a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments.\n",
    "\n",
    "**NOTE**: If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "\n",
    "**NOTE**: This model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9e9e",
   "metadata": {},
   "source": [
    "### Example prompts\n",
    "***\n",
    "The examples in this section demonstrate how to perform text generation with conversational dialog as prompt inputs. Example payloads are retrieved programmatically from the `JumpStartModel` object.\n",
    "\n",
    "Input messages for Llama-2 chat models should exhibit the following format. The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...). The last message must be from 'user'. A simple user prompt may look like the following:\n",
    "```\n",
    "<s>[INST] {user_prompt} [/INST]\n",
    "```\n",
    "You may also add a system prompt with the following syntax:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_prompt} [/INST]\n",
    "```\n",
    "Finally, you can have a conversational interaction with the model by including all previous user prompts and assistant responses in the input:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_prompt_1} [/INST] {assistant_response_1} </s><s>[INST] {user_prompt_1} [/INST]\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b11b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example_payloads = model.retrieve_all_examples()\n",
    "\n",
    "# for payload in example_payloads[:1]:\n",
    "#     response = predictor.predict(payload.body)\n",
    "#     print(\"\\nInput\\n\", payload.body, \"\\n\\nOutput\\n\", response[0][\"generated_text\"], \"\\n\\n===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f1ab1-a304-4e3c-99ee-cd51a7795f75",
   "metadata": {},
   "source": [
    "# lasson专用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e024dd-2e12-4da3-a31b-30377da889fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 相似度计算方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c351c54-0c44-4661-9f45-e5a5020266b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lasson download bleu\n",
    "# !pip install nltk sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f7a28-e94a-49d7-a39d-bb49d9320e06",
   "metadata": {},
   "source": [
    "## bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2c8c0-a632-4e45-ae51-0998308b295d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacrebleu 2.4.2\n"
     ]
    }
   ],
   "source": [
    "!sacrebleu --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab7e76-d67d-4c74-a7b8-f407e756f3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score (): 100.00000000000004\n",
      "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 35 ref_len = 35)\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# 参考文本（字符串）\n",
    "reference_text = \"The patient is referred due to the rapid progression of the tumor despite initial treatment modalities. Further evaluation and management from a specialized neuro-oncology team are essential for this recurrent and aggressive tumor.\"\n",
    "\n",
    "# 生成的文本（字符串）\n",
    "candidate_text = \"The patient is referred due to the rapid progression of the tumor despite initial treatment modalities. Further evaluation and management from a specialized neuro-oncology team are essential for this recurrent and aggressive tumor.\"\n",
    "\n",
    "# 计算BLEU分数，调整n-gram权重\n",
    "# 默认权重是(0.25, 0.25, 0.25, 0.25)，分别对应1-gram到4-gram\n",
    "# 我们可以调整权重，例如只考虑1-gram和2-gram，权重分别为(0.5, 0.5)\n",
    "# 设置n-gram权重\n",
    "weights = (0.5, 0.5, 0, 0)\n",
    "\n",
    "# 创建BLEU对象，并传入自定义的权重\n",
    "weights = (0.5, 0.5, 0, 0)\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "print(f\"BLEU score (): {bleu.score}\")\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ae564-194a-4b95-8b7b-ffcbbffec319",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5de194-999d-4130-86dd-ff07124a4ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d70113-6594-4263-9a92-46c841d64f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores: {'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# 计算ROUGE分数\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(candidate_text, reference_text, avg=True) # 由于只有一个 reference，所以 avg没有影响\n",
    "print(f\"ROUGE scores: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c2bf9-3e73-463c-98c2-9e417a267c91",
   "metadata": {},
   "source": [
    "# deploy llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c0b1d0-68c8-4eef-9cb3-c1361e5ba728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " {\"referral_content\": \"During a routine eye examination, we noted significant disc swelling in both eyes. The referral is due to the presentation of disc swelling in both eyes. This finding was corroborated by the presence of blurred disc margins and swollen discs on fundoscopy. Given these concerning features, further diagnostic workup and specialized management are warranted to rule out underlying pathologies such as intracranial hypertension or other optic neuropathies.\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lasson do\n",
    "with open(\"../letter.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    letter = f.read()\n",
    "# print(letter)\n",
    "\n",
    "response = predictor.predict({'inputs': letter,\n",
    "                             'parameters': {'max_new_tokens': 128}})\n",
    "\n",
    "print(\"Output:\\n\", response[0][\"generated_text\"].strip(), end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590b7c3-976d-4893-95eb-293c0a6ac0b9",
   "metadata": {},
   "source": [
    "## test all test.json with llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69ff2f49-ab87-472e-bb6d-db0dd3c05d39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from rouge import Rouge\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "def replace_output_prefix(input_str):\n",
    "    # 使用正则表达式进行替换，忽略大小写\n",
    "    result = re.sub(r'(?i)^Output:\\s*', '', input_str)\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_jsonl_with_llama2(predictor, path, output_jsonl_path, csv_file_path):\n",
    "    test_data_json = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data_json.append(json.loads(line.strip()))\n",
    "    rouge_score_list = []\n",
    "    bleu_score_list = []\n",
    "\n",
    "    rouge = Rouge()\n",
    "\n",
    "    evaluate_list = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        instruction = single_test[\"instruction\"]\n",
    "        whole_letter = single_test[\"whole_letter\"]\n",
    "        referral_content = single_test[\"referral_content\"]\n",
    "        prompt = f\"{instruction}\\n\\n###\\n\\n{whole_letter}\\n\\n###\"\n",
    "        response = predictor.predict({'inputs': prompt,\n",
    "                                 'parameters': {'max_new_tokens': 256}})\n",
    "        # print(prompt)\n",
    "        reference_text = referral_content\n",
    "        try:\n",
    "            tmp_str = replace_output_prefix(response[0][\"generated_text\"].strip())\n",
    "            tmp = json.loads(tmp_str)\n",
    "            candidate_text = tmp[\"referral_content\"]\n",
    "        except Exception as err:\n",
    "            print(single_test[\"id\"])\n",
    "            print(response[0][\"generated_text\"].strip())\n",
    "            print()\n",
    "            candidate_text = \"extract failure\"\n",
    "        finally:\n",
    "\n",
    "            evaluate_list.append(candidate_text)\n",
    "            # print(\"predict: \" + candidate_text)\n",
    "            # print(\"real: \" + reference_text)\n",
    "\n",
    "\n",
    "            bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "            bleu_score_list.append(bleu.score)\n",
    "            # print(bleu.score)\n",
    "            single_test[\"bleu\"] = bleu.score\n",
    "            single_test[\"predict_referral_content\"] = candidate_text\n",
    "            # print()\n",
    "            # 计算ROUGE分数\n",
    "            scores = rouge.get_scores(candidate_text, reference_text) # 由于只有一个 reference，所以 avg没有影响\n",
    "            rouge_score_list.append(scores)\n",
    "            \n",
    "#     with open(output_jsonl_path, mode='w', encoding='utf-8') as f:\n",
    "#         for single_test in test_data_json:\n",
    "#             f.write(json.dumps(single_test, ensure_ascii=False) + '\\n')\n",
    "\n",
    "#     print(f\"predicted data has been saved to {output_path}.\")\n",
    "    \n",
    "    # 创建 CSV 文件\n",
    "    csv_data = []\n",
    "\n",
    "    for single_test in test_data_json:\n",
    "        csv_data.append({\n",
    "            \"id\": single_test[\"id\"],\n",
    "            \"name\": single_test[\"name\"],\n",
    "            \"instruction\": single_test[\"instruction\"],\n",
    "            \"whole_letter\": single_test[\"whole_letter\"],\n",
    "            \"referral_content\": single_test[\"referral_content\"],\n",
    "            \"predict_referral_content\": single_test[\"predict_referral_content\"],\n",
    "            \"bleu\": single_test[\"bleu\"],\n",
    "        })\n",
    "\n",
    "    # 创建 DataFrame\n",
    "    df = pd.DataFrame(csv_data)\n",
    "\n",
    "    # 保存为 CSV 文件\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"CSV file has been saved to {csv_file_path}\")\n",
    "    \n",
    "    return evaluate_list, bleu_score_list, rouge_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e9107b4-f214-42d0-9f50-e1e3f154b9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been saved to ../test_dir/predict_llama2_7b_f_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_list, test_bleu_score_list,test_rouge_score_list = evaluate_jsonl_with_llama2(predictor, \n",
    "                                        \"../test_dir/test.jsonl\", \"../test_dir/predict_llama2_7b_f_test.jsonl\",\n",
    "                                                                \"../test_dir/predict_llama2_7b_f_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf805980-8c39-4148-8e34-03c254e6cfba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been saved to ../train_test_data/predict_llama2_7b_f_train.csv\n"
     ]
    }
   ],
   "source": [
    "train_evaluate_list, train_bleu_score_list,train_rouge_score_list = evaluate_jsonl_with_llama2(predictor,\n",
    "                                                                \"../train_test_data/train.jsonl\", \"../train_test_data/predict_llama2_7b_f_train.jsonl\",\n",
    "                                                                \"../train_test_data/predict_llama2_7b_f_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "464dddd9-55c2-423f-b53f-3b941bc3ca67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_predict_data(bleu_score_list):\n",
    "    # 统计大于100的个数\n",
    "    count_gt_100 = sum(1 for score in bleu_score_list if score >= 100)\n",
    "\n",
    "    # 统计大于70的个数\n",
    "    count_gt_70 = sum(1 for score in bleu_score_list if score > 70)\n",
    "\n",
    "    prob_gt_100 = count_gt_100 / len(bleu_score_list)\n",
    "    prob_gt_70 = count_gt_70 / len(bleu_score_list)\n",
    "    average_score = sum(bleu_score_list) / float(len(bleu_score_list))\n",
    "\n",
    "    print(f\"分数大于100的个数：{count_gt_100}, 占所有数据的百分比为： {prob_gt_100}\")\n",
    "    print(f\"分数大于70的个数：{count_gt_70}, 占所有数据的百分比为： {prob_gt_70}\")\n",
    "    print(f\"bleu平均分数: {average_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95ca531f-9b93-4ae8-8c63-e523cb200e06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分数大于100的个数：43, 占所有数据的百分比为： 0.524390243902439\n",
      "分数大于70的个数：54, 占所有数据的百分比为： 0.6585365853658537\n",
      "bleu平均分数: 74.19628800060674\n"
     ]
    }
   ],
   "source": [
    "analyze_predict_data(train_bleu_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f96f918-a7ef-49a4-9fb6-98d1cdf58583",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分数大于100的个数：10, 占所有数据的百分比为： 0.47619047619047616\n",
      "分数大于70的个数：11, 占所有数据的百分比为： 0.5238095238095238\n",
      "bleu平均分数: 62.56950311883414\n"
     ]
    }
   ],
   "source": [
    "analyze_predict_data(test_bleu_score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558fea8",
   "metadata": {},
   "source": [
    "***\n",
    "While not used in the previously provided example payloads, you can format your own messages to the Llama-2 model with the following utility function.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5adf9b4-c7e1-4090-aefe-9cae0d096968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format messages for Llama-2 chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join([\"<<SYS>>\\n\", messages[0][\"content\"], \"\\n<</SYS>>\\n\\n\", messages[1][\"content\"]])\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend([\"<s>\", \"[INST] \", (user[\"content\"]).strip(), \" [/INST] \", (answer[\"content\"]).strip(), \"</s>\"])\n",
    "\n",
    "    prompt.extend([\"<s>\", \"[INST] \", (messages[-1][\"content\"]).strip(), \" [/INST] \"])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "dialog = [\n",
    "    {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n",
    "    {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "]\n",
    "\n",
    "prompt = format_messages(dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24cc5560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c6e59-2172-4349-9c29-20b5015bde7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
